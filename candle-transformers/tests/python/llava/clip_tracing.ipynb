{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip.processing_clip import CLIPProcessor\n",
    "\n",
    "LLAVA_IMAGE_ENCODER = \"openai/clip-vit-large-patch14-336\"\n",
    "LLAVA_IMAGE_ENCODER_SMALL = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(LLAVA_IMAGE_ENCODER_SMALL)\n",
    "processor: CLIPProcessor = AutoProcessor.from_pretrained(LLAVA_IMAGE_ENCODER_SMALL)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(processor)\n",
    "type(processor).__bases__\n",
    "\n",
    "processor.model_input_names\n",
    "processor.image_processor_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor\n",
    "type(processor.image_processor).__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_processing_utils import BaseImageProcessor\n",
    "type(BaseImageProcessor).__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "pixels = inputs.pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPVisionModel, CLIPVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model\n",
    "for base in m.base_model.children():\n",
    "    print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "l = nn.Linear(2, 2)\n",
    "net = nn.Sequential(l, l)\n",
    "for idx, m in enumerate(net.named_modules()):\n",
    "    print(idx, '->', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(model.named_modules(remove_duplicate=True))\n",
    "modules_dups = dict(model.named_modules(remove_duplicate=False))\n",
    "params = dict(model.named_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(modules), len(modules_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"a\" * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels.toli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = dict(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(children['vision_model'].named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(model.named_modules(remove_duplicate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modules\")\n",
    "modules_list = []\n",
    "for i, (k, m) in enumerate(model.named_modules()):\n",
    "    module_cls = m.__class__.__name__\n",
    "    modules_list.append((k, module_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "params_dict = {}\n",
    "params_list = []\n",
    "for i, (k, p) in enumerate(model.named_parameters()):\n",
    "    parent_module = '.'.join(k.split('.')[:-1])\n",
    "    module_cls = modules[parent_module].__class__.__name__\n",
    "    params_list.append((k, module_cls, list(p.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(modules['vision_model'].__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ParamInfo:\n",
    "    path: str\n",
    "    shape: list\n",
    "    dtype: Optional[torch.dtype] = None\n",
    "    def __repr__(self) -> str:\n",
    "        s = f'Param: \\\"{self.path}\\\", Shape: {self.shape}'\n",
    "        return s +  f\"dtype: {self.dtype}\" if self.dtype is not None else s\n",
    "\n",
    "@dataclass\n",
    "class ModulePath:\n",
    "    path: str\n",
    "    module_name: str\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.path} = {self.module_name}\"\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.path + self.module_name)\n",
    "        \n",
    "@dataclass\n",
    "class TensorMap:\n",
    "    path: str\n",
    "    module_type: str\n",
    "    children: dict    \n",
    "    \n",
    "def modules_to_implement(m_dict):\n",
    "    child_modules = []\n",
    "    if isinstance(m_dict, list):\n",
    "        return m_dict\n",
    "    \n",
    "    for modulepath, child in m_dict.items():\n",
    "        print(f\"{modulepath.path}: {modulepath.module_name}\")\n",
    "        child_modules.append(modules_to_implement(child))\n",
    "    \n",
    "    return child_modules\n",
    "\n",
    "def walk_modules(module, prefix=''):\n",
    "    module_dict = {}\n",
    "    if len(list(module.children())) == 0:\n",
    "        params = [ParamInfo(path='.'.join([prefix,k]), shape=list(v.shape)) for k,v in module.named_parameters()]\n",
    "        return params\n",
    "    \n",
    "    for n,m in module.named_children():\n",
    "        m_cls = m.__class__.__name__\n",
    "        path = '.'.join([prefix, n]) if prefix else n\n",
    "        module_dict[ModulePath(path, m_cls)] = walk_modules(m, prefix=path) \n",
    "    return module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict = walk_modules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(m_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_modules = modules_to_implement(m_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'vision_model',\n",
       " 'vision_model.embeddings',\n",
       " 'vision_model.embeddings.patch_embedding',\n",
       " 'vision_model.embeddings.position_embedding']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(modules.keys())\n",
    "keys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _make_module_map(model: torch.nn.Module):\n",
    "    module_map = {}\n",
    "\n",
    "    for k,v in model.named_modules():\n",
    "        module_cls = v.__class__.__name__\n",
    "        if module_cls in module_map:\n",
    "            module_map[module_cls] = module_map[module_cls] + [k]\n",
    "        else:\n",
    "            module_map[module_cls] = [k]\n",
    "        \n",
    "    return module_map\n",
    "\n",
    "def get_module(name: str, model: torch.nn.Module):\n",
    "    module_map = _make_module_map(model)    \n",
    "    modules = dict(model.named_modules())\n",
    "    \n",
    "    path = module_map[name]\n",
    "    \n",
    "    key = path[0]    \n",
    "    fmt_path = path[0] + ', ... ,' + path[-1] if len(path) > 1 else path[0]\n",
    "         \n",
    "    module = modules.get(key)\n",
    "    print(f\"{fmt_path}: {module}\")\n",
    "    \n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_model.encoder.layers.0.self_attn, ... ,vision_model.encoder.layers.11.self_attn: CLIPAttention(\n",
      "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPAttention(\n",
       "  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_module('CLIPAttention', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': CLIPVisionEmbeddings(\n",
       "   (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "   (position_embedding): Embedding(50, 768)\n",
       " ),\n",
       " 'patch_embedding': Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False),\n",
       " 'position_embedding': Embedding(50, 768)}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(m.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(model.named_modules())\n",
    "params = dict(model.named_parameters())\n",
    "\n",
    "type(modules['vision_model.embeddings'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ident = \" \" * 4\n",
    "for module_name, module in model.named_modules():\n",
    "    if module_name:\n",
    "        print(f\"MODULE {module_name}\")\n",
    "        \n",
    "        for p_name, p in module.named_parameters():\n",
    "            print(f\"{ident}PARAM {p_name} {list(p.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k,v in params.items():\n",
    "    depth = len(k.split('.')) - 1\n",
    "    ident = depth * \"  \"    \n",
    "    print(f\"{depth} {ident}{k} {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "tensors, stats = summary(model, input_data=[pixels], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
