{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip.processing_clip import CLIPProcessor\n",
    "\n",
    "LLAVA_IMAGE_ENCODER = \"openai/clip-vit-large-patch14-336\"\n",
    "LLAVA_IMAGE_ENCODER_SMALL = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(LLAVA_IMAGE_ENCODER_SMALL)\n",
    "processor: CLIPProcessor = AutoProcessor.from_pretrained(LLAVA_IMAGE_ENCODER_SMALL)\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.processing_clip.CLIPProcessor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(transformers.processing_utils.ProcessorMixin,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'pixel_values']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'CLIPImageProcessor'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processor)\n",
    "type(processor).__bases__\n",
    "\n",
    "processor.model_input_names\n",
    "processor.image_processor_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.image_processor\n",
    "type(processor.image_processor).__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_processing_utils import BaseImageProcessor\n",
    "type(BaseImageProcessor).__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "pixels = inputs.pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPVisionModel, CLIPVisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model\n",
    "for base in m.base_model.children():\n",
    "    print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "l = nn.Linear(2, 2)\n",
    "net = nn.Sequential(l, l)\n",
    "for idx, m in enumerate(net.named_modules()):\n",
    "    print(idx, '->', m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(model.named_modules(remove_duplicate=True))\n",
    "modules_dups = dict(model.named_modules(remove_duplicate=False))\n",
    "params = dict(model.named_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 153)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modules), len(modules_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaaaa'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a\" * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5873,  0.5873,  0.6165,  ...,  0.0617,  0.0471, -0.0259],\n",
       "          [ 0.5727,  0.5727,  0.6603,  ...,  0.1201,  0.0763,  0.0909],\n",
       "          [ 0.5873,  0.5435,  0.6165,  ...,  0.0325,  0.1201,  0.0617],\n",
       "          ...,\n",
       "          [ 1.8719,  1.8573,  1.8719,  ...,  1.3902,  1.4340,  1.4194],\n",
       "          [ 1.8281,  1.8719,  1.8427,  ...,  1.4486,  1.4340,  1.5070],\n",
       "          [ 1.8573,  1.9011,  1.8281,  ...,  1.3756,  1.3610,  1.4486]],\n",
       "\n",
       "         [[-1.3169, -1.3019, -1.3169,  ..., -1.4970, -1.4369, -1.4820],\n",
       "          [-1.2418, -1.2718, -1.2268,  ..., -1.4369, -1.4669, -1.4519],\n",
       "          [-1.2568, -1.3169, -1.2268,  ..., -1.4669, -1.4069, -1.4519],\n",
       "          ...,\n",
       "          [ 0.1239,  0.1089,  0.1239,  ..., -0.7016, -0.6865, -0.6865],\n",
       "          [ 0.0789,  0.0939,  0.0488,  ..., -0.6565, -0.6865, -0.6115],\n",
       "          [ 0.0939,  0.1089,  0.0038,  ..., -0.7766, -0.7316, -0.6115]],\n",
       "\n",
       "         [[-0.4848, -0.4137, -0.3853,  ..., -0.9541, -0.8545, -0.8545],\n",
       "          [-0.4137, -0.4706, -0.3711,  ..., -0.8119, -0.8545, -0.7834],\n",
       "          [-0.3284, -0.4422, -0.3853,  ..., -0.8688, -0.8119, -0.8830],\n",
       "          ...,\n",
       "          [ 1.5771,  1.6482,  1.6340,  ...,  0.9088,  0.9514,  0.8945],\n",
       "          [ 1.6198,  1.6055,  1.6055,  ...,  0.8661,  0.8092,  0.7950],\n",
       "          [ 1.6624,  1.6766,  1.5487,  ...,  0.7950,  0.8661,  0.8519]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels.toli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = dict(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': CLIPVisionEmbeddings(\n",
       "   (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "   (position_embedding): Embedding(50, 768)\n",
       " ),\n",
       " 'pre_layrnorm': LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
       " 'encoder': CLIPEncoder(\n",
       "   (layers): ModuleList(\n",
       "     (0-11): 12 x CLIPEncoderLayer(\n",
       "       (self_attn): CLIPAttention(\n",
       "         (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "       )\n",
       "       (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "       (mlp): CLIPMLP(\n",
       "         (activation_fn): QuickGELUActivation()\n",
       "         (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "         (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "       )\n",
       "       (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'post_layernorm': LayerNorm((768,), eps=1e-05, elementwise_affine=True)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(children['vision_model'].named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 224, 224]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = dict(model.named_modules(remove_duplicate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionModel(\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "      (position_embedding): Embedding(50, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(str(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules\n"
     ]
    }
   ],
   "source": [
    "print(\"Modules\")\n",
    "modules_list = []\n",
    "for i, (k, m) in enumerate(model.named_modules()):\n",
    "    module_cls = m.__class__.__name__\n",
    "    modules_list.append((k, module_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'CLIPVisionModel'),\n",
       " ('vision_model', 'CLIPVisionTransformer'),\n",
       " ('vision_model.embeddings', 'CLIPVisionEmbeddings'),\n",
       " ('vision_model.embeddings.patch_embedding', 'Conv2d'),\n",
       " ('vision_model.embeddings.position_embedding', 'Embedding'),\n",
       " ('vision_model.pre_layrnorm', 'LayerNorm'),\n",
       " ('vision_model.encoder', 'CLIPEncoder'),\n",
       " ('vision_model.encoder.layers', 'ModuleList'),\n",
       " ('vision_model.encoder.layers.0', 'CLIPEncoderLayer'),\n",
       " ('vision_model.encoder.layers.0.self_attn', 'CLIPAttention')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "params_dict = {}\n",
    "params_list = []\n",
    "for i, (k, p) in enumerate(model.named_parameters()):\n",
    "    parent_module = '.'.join(k.split('.')[:-1])\n",
    "    module_cls = modules[parent_module].__class__.__name__\n",
    "    params_list.append((k, module_cls, list(p.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<class 'transformers.models.clip.modeling_clip.CLIPVisionTransformer'>\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(modules['vision_model'].__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class ParamInfo:\n",
    "    path: str\n",
    "    shape: list\n",
    "    dtype: Optional[torch.dtype] = None\n",
    "    def __repr__(self) -> str:\n",
    "        s = f'Param: \\\"{self.path}\\\", Shape: {self.shape}'\n",
    "        return s +  f\"dtype: {self.dtype}\" if self.dtype is not None else s\n",
    "    \n",
    "@dataclass\n",
    "class TensorMap:\n",
    "    path: str\n",
    "    module_type: str\n",
    "    children: dict    \n",
    "    \n",
    "def walk_modules(module, prefix=''):\n",
    "    module_dict = {}\n",
    "    if len(list(module.children())) == 0:\n",
    "        params = [ParamInfo(path='.'.join([prefix,k]), shape=list(v.shape)) for k,v in module.named_parameters()]\n",
    "        return params\n",
    "    \n",
    "    for n,m in module.named_children():\n",
    "        m_cls = m.__class__.__name__\n",
    "        path = '.'.join([prefix, n]) if prefix else n\n",
    "        module_dict[(path, m_cls)] = walk_modules(m, prefix=path) \n",
    "    return module_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict = walk_modules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(m_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('vision_model.embeddings',\n",
       "  'CLIPVisionEmbeddings'): {('vision_model.embeddings.patch_embedding',\n",
       "   'Conv2d'): [Param: \"vision_model.embeddings.patch_embedding.weight\", Shape: [768, 3, 32, 32]], ('vision_model.embeddings.position_embedding',\n",
       "   'Embedding'): [Param: \"vision_model.embeddings.position_embedding.weight\", Shape: [50, 768]]},\n",
       " ('vision_model.pre_layrnorm',\n",
       "  'LayerNorm'): [Param: \"vision_model.pre_layrnorm.weight\", Shape: [768], Param: \"vision_model.pre_layrnorm.bias\", Shape: [768]],\n",
       " ('vision_model.encoder',\n",
       "  'CLIPEncoder'): {('vision_model.encoder.layers',\n",
       "   'ModuleList'): {('vision_model.encoder.layers.0',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.0.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.0.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.0.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.0.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.0.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.0.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.0.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.0.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.0.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.0.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.0.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.0.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.0.self_attn.out_proj.bias\", Shape: [768]]}, ('vision_model.encoder.layers.0.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.0.layer_norm1.weight\", Shape: [768],\n",
       "     Param: \"vision_model.encoder.layers.0.layer_norm1.bias\", Shape: [768]], ('vision_model.encoder.layers.0.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.0.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [],\n",
       "     ('vision_model.encoder.layers.0.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.0.mlp.fc1.weight\", Shape: [3072, 768], Param: \"vision_model.encoder.layers.0.mlp.fc1.bias\", Shape: [3072]],\n",
       "     ('vision_model.encoder.layers.0.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.0.mlp.fc2.weight\", Shape: [768, 3072], Param: \"vision_model.encoder.layers.0.mlp.fc2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.0.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.0.layer_norm2.weight\", Shape: [768],\n",
       "     Param: \"vision_model.encoder.layers.0.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.1',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.1.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.1.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.1.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.1.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.1.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.1.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.1.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.1.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.1.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.1.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.1.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.1.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.1.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.1.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.1.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.1.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.1.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.1.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.1.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.1.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.1.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.1.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.1.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.1.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.1.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.1.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.1.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.2',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.2.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.2.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.2.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.2.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.2.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.2.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.2.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.2.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.2.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.2.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.2.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.2.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.2.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.2.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.2.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.2.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.2.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.2.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.2.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.2.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.2.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.2.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.2.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.2.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.2.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.2.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.2.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.3',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.3.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.3.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.3.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.3.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.3.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.3.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.3.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.3.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.3.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.3.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.3.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.3.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.3.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.3.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.3.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.3.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.3.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.3.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.3.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.3.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.3.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.3.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.3.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.3.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.3.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.3.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.3.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.4',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.4.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.4.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.4.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.4.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.4.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.4.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.4.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.4.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.4.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.4.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.4.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.4.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.4.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.4.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.4.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.4.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.4.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.4.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.4.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.4.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.4.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.4.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.4.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.4.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.4.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.4.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.4.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.5',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.5.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.5.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.5.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.5.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.5.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.5.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.5.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.5.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.5.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.5.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.5.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.5.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.5.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.5.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.5.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.5.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.5.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.5.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.5.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.5.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.5.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.5.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.5.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.5.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.5.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.5.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.5.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.6',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.6.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.6.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.6.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.6.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.6.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.6.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.6.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.6.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.6.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.6.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.6.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.6.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.6.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.6.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.6.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.6.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.6.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.6.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.6.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.6.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.6.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.6.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.6.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.6.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.6.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.6.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.6.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.7',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.7.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.7.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.7.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.7.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.7.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.7.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.7.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.7.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.7.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.7.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.7.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.7.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.7.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.7.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.7.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.7.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.7.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.7.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.7.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.7.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.7.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.7.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.7.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.7.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.7.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.7.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.7.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.8',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.8.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.8.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.8.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.8.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.8.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.8.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.8.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.8.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.8.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.8.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.8.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.8.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.8.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.8.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.8.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.8.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.8.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.8.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.8.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.8.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.8.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.8.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.8.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.8.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.8.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.8.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.8.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.9',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.9.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.9.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.9.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.9.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.9.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.9.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.9.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.9.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.9.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.9.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.9.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.9.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.9.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.9.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.9.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.9.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.9.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.9.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.9.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.9.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.9.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.9.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.9.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.9.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.9.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.9.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.9.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.10',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.10.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.10.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.10.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.10.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.10.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.10.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.10.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.10.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.10.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.10.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.10.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.10.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.10.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.10.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.10.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.10.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.10.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.10.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.10.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.10.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.10.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.10.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.10.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.10.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.10.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.10.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.10.layer_norm2.bias\", Shape: [768]]}, ('vision_model.encoder.layers.11',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.11.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.11.self_attn.k_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.11.self_attn.k_proj.weight\", Shape: [768, 768],\n",
       "      Param: \"vision_model.encoder.layers.11.self_attn.k_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.11.self_attn.v_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.11.self_attn.v_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.11.self_attn.v_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.11.self_attn.q_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.11.self_attn.q_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.11.self_attn.q_proj.bias\", Shape: [768]],\n",
       "     ('vision_model.encoder.layers.11.self_attn.out_proj',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.11.self_attn.out_proj.weight\", Shape: [768, 768], Param: \"vision_model.encoder.layers.11.self_attn.out_proj.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.11.layer_norm1',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.11.layer_norm1.weight\", Shape: [768], Param: \"vision_model.encoder.layers.11.layer_norm1.bias\", Shape: [768]],\n",
       "    ('vision_model.encoder.layers.11.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.11.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): [], ('vision_model.encoder.layers.11.mlp.fc1',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.11.mlp.fc1.weight\", Shape: [3072, 768],\n",
       "      Param: \"vision_model.encoder.layers.11.mlp.fc1.bias\", Shape: [3072]], ('vision_model.encoder.layers.11.mlp.fc2',\n",
       "      'Linear'): [Param: \"vision_model.encoder.layers.11.mlp.fc2.weight\", Shape: [768, 3072],\n",
       "      Param: \"vision_model.encoder.layers.11.mlp.fc2.bias\", Shape: [768]]},\n",
       "    ('vision_model.encoder.layers.11.layer_norm2',\n",
       "     'LayerNorm'): [Param: \"vision_model.encoder.layers.11.layer_norm2.weight\", Shape: [768], Param: \"vision_model.encoder.layers.11.layer_norm2.bias\", Shape: [768]]}}},\n",
       " ('vision_model.post_layernorm',\n",
       "  'LayerNorm'): [Param: \"vision_model.post_layernorm.weight\", Shape: [768], Param: \"vision_model.post_layernorm.bias\", Shape: [768]]}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_dict[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('vision_model.embeddings',\n",
       "  'CLIPVisionEmbeddings'): {('vision_model.embeddings.patch_embedding',\n",
       "   'Conv2d'): {'vision_model.embeddings.patch_embedding.weight': [768,\n",
       "    3,\n",
       "    32,\n",
       "    32]}, ('vision_model.embeddings.position_embedding',\n",
       "   'Embedding'): {'vision_model.embeddings.position_embedding.weight': [50,\n",
       "    768]}},\n",
       " ('vision_model.pre_layrnorm',\n",
       "  'LayerNorm'): {'vision_model.pre_layrnorm.weight': [768], 'vision_model.pre_layrnorm.bias': [768]},\n",
       " ('vision_model.encoder',\n",
       "  'CLIPEncoder'): {('vision_model.encoder.layers',\n",
       "   'ModuleList'): {('vision_model.encoder.layers.0',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.0.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.0.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.0.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.0.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.0.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.0.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.0.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.0.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.0.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.0.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.0.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.0.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.0.self_attn.out_proj.bias': [768]}}, ('vision_model.encoder.layers.0.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.0.layer_norm1.weight': [768],\n",
       "     'vision_model.encoder.layers.0.layer_norm1.bias': [768]}, ('vision_model.encoder.layers.0.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.0.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {},\n",
       "     ('vision_model.encoder.layers.0.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.0.mlp.fc1.weight': [3072,\n",
       "       768], 'vision_model.encoder.layers.0.mlp.fc1.bias': [3072]},\n",
       "     ('vision_model.encoder.layers.0.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.0.mlp.fc2.weight': [768,\n",
       "       3072], 'vision_model.encoder.layers.0.mlp.fc2.bias': [768]}}, ('vision_model.encoder.layers.0.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.0.layer_norm2.weight': [768],\n",
       "     'vision_model.encoder.layers.0.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.1',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.1.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.1.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.1.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.1.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.1.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.1.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.1.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.1.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.1.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.1.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.1.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.1.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.1.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.1.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.1.layer_norm1.weight': [768], 'vision_model.encoder.layers.1.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.1.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.1.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.1.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.1.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.1.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.1.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.1.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.1.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.1.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.1.layer_norm2.weight': [768], 'vision_model.encoder.layers.1.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.2',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.2.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.2.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.2.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.2.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.2.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.2.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.2.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.2.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.2.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.2.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.2.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.2.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.2.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.2.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.2.layer_norm1.weight': [768], 'vision_model.encoder.layers.2.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.2.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.2.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.2.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.2.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.2.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.2.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.2.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.2.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.2.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.2.layer_norm2.weight': [768], 'vision_model.encoder.layers.2.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.3',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.3.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.3.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.3.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.3.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.3.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.3.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.3.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.3.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.3.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.3.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.3.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.3.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.3.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.3.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.3.layer_norm1.weight': [768], 'vision_model.encoder.layers.3.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.3.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.3.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.3.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.3.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.3.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.3.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.3.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.3.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.3.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.3.layer_norm2.weight': [768], 'vision_model.encoder.layers.3.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.4',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.4.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.4.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.4.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.4.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.4.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.4.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.4.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.4.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.4.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.4.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.4.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.4.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.4.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.4.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.4.layer_norm1.weight': [768], 'vision_model.encoder.layers.4.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.4.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.4.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.4.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.4.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.4.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.4.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.4.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.4.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.4.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.4.layer_norm2.weight': [768], 'vision_model.encoder.layers.4.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.5',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.5.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.5.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.5.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.5.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.5.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.5.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.5.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.5.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.5.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.5.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.5.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.5.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.5.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.5.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.5.layer_norm1.weight': [768], 'vision_model.encoder.layers.5.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.5.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.5.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.5.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.5.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.5.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.5.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.5.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.5.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.5.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.5.layer_norm2.weight': [768], 'vision_model.encoder.layers.5.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.6',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.6.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.6.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.6.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.6.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.6.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.6.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.6.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.6.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.6.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.6.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.6.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.6.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.6.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.6.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.6.layer_norm1.weight': [768], 'vision_model.encoder.layers.6.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.6.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.6.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.6.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.6.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.6.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.6.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.6.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.6.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.6.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.6.layer_norm2.weight': [768], 'vision_model.encoder.layers.6.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.7',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.7.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.7.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.7.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.7.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.7.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.7.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.7.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.7.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.7.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.7.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.7.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.7.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.7.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.7.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.7.layer_norm1.weight': [768], 'vision_model.encoder.layers.7.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.7.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.7.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.7.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.7.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.7.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.7.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.7.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.7.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.7.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.7.layer_norm2.weight': [768], 'vision_model.encoder.layers.7.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.8',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.8.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.8.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.8.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.8.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.8.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.8.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.8.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.8.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.8.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.8.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.8.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.8.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.8.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.8.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.8.layer_norm1.weight': [768], 'vision_model.encoder.layers.8.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.8.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.8.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.8.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.8.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.8.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.8.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.8.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.8.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.8.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.8.layer_norm2.weight': [768], 'vision_model.encoder.layers.8.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.9',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.9.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.9.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.9.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.9.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.9.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.9.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.9.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.9.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.9.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.9.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.9.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.9.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.9.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.9.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.9.layer_norm1.weight': [768], 'vision_model.encoder.layers.9.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.9.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.9.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.9.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.9.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.9.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.9.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.9.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.9.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.9.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.9.layer_norm2.weight': [768], 'vision_model.encoder.layers.9.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.10',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.10.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.10.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.10.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.10.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.10.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.10.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.10.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.10.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.10.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.10.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.10.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.10.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.10.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.10.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.10.layer_norm1.weight': [768], 'vision_model.encoder.layers.10.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.10.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.10.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.10.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.10.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.10.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.10.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.10.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.10.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.10.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.10.layer_norm2.weight': [768], 'vision_model.encoder.layers.10.layer_norm2.bias': [768]}}, ('vision_model.encoder.layers.11',\n",
       "    'CLIPEncoderLayer'): {('vision_model.encoder.layers.11.self_attn',\n",
       "     'CLIPAttention'): {('vision_model.encoder.layers.11.self_attn.k_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.11.self_attn.k_proj.weight': [768,\n",
       "       768],\n",
       "      'vision_model.encoder.layers.11.self_attn.k_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.11.self_attn.v_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.11.self_attn.v_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.11.self_attn.v_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.11.self_attn.q_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.11.self_attn.q_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.11.self_attn.q_proj.bias': [768]},\n",
       "     ('vision_model.encoder.layers.11.self_attn.out_proj',\n",
       "      'Linear'): {'vision_model.encoder.layers.11.self_attn.out_proj.weight': [768,\n",
       "       768], 'vision_model.encoder.layers.11.self_attn.out_proj.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.11.layer_norm1',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.11.layer_norm1.weight': [768], 'vision_model.encoder.layers.11.layer_norm1.bias': [768]},\n",
       "    ('vision_model.encoder.layers.11.mlp',\n",
       "     'CLIPMLP'): {('vision_model.encoder.layers.11.mlp.activation_fn',\n",
       "      'QuickGELUActivation'): {}, ('vision_model.encoder.layers.11.mlp.fc1',\n",
       "      'Linear'): {'vision_model.encoder.layers.11.mlp.fc1.weight': [3072, 768],\n",
       "      'vision_model.encoder.layers.11.mlp.fc1.bias': [3072]}, ('vision_model.encoder.layers.11.mlp.fc2',\n",
       "      'Linear'): {'vision_model.encoder.layers.11.mlp.fc2.weight': [768, 3072],\n",
       "      'vision_model.encoder.layers.11.mlp.fc2.bias': [768]}},\n",
       "    ('vision_model.encoder.layers.11.layer_norm2',\n",
       "     'LayerNorm'): {'vision_model.encoder.layers.11.layer_norm2.weight': [768], 'vision_model.encoder.layers.11.layer_norm2.bias': [768]}}}},\n",
       " ('vision_model.post_layernorm',\n",
       "  'LayerNorm'): {'vision_model.post_layernorm.weight': [768], 'vision_model.post_layernorm.bias': [768]}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_dict[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vision_model.embeddings.class_embedding', 'CLIPVisionEmbeddings', [768]),\n",
       " ('vision_model.embeddings.patch_embedding.weight',\n",
       "  'Conv2d',\n",
       "  [768, 3, 32, 32]),\n",
       " ('vision_model.embeddings.position_embedding.weight', 'Embedding', [50, 768]),\n",
       " ('vision_model.pre_layrnorm.weight', 'LayerNorm', [768]),\n",
       " ('vision_model.pre_layrnorm.bias', 'LayerNorm', [768]),\n",
       " ('vision_model.encoder.layers.0.self_attn.k_proj.weight',\n",
       "  'Linear',\n",
       "  [768, 768]),\n",
       " ('vision_model.encoder.layers.0.self_attn.k_proj.bias', 'Linear', [768]),\n",
       " ('vision_model.encoder.layers.0.self_attn.v_proj.weight',\n",
       "  'Linear',\n",
       "  [768, 768]),\n",
       " ('vision_model.encoder.layers.0.self_attn.v_proj.bias', 'Linear', [768]),\n",
       " ('vision_model.encoder.layers.0.self_attn.q_proj.weight',\n",
       "  'Linear',\n",
       "  [768, 768])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.modeling_clip.CLIPVisionEmbeddings"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = dict(model.named_modules())\n",
    "params = dict(model.named_parameters())\n",
    "\n",
    "type(modules['vision_model.embeddings'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ident = \" \" * 4\n",
    "for module_name, module in model.named_modules():\n",
    "    if module_name:\n",
    "        print(f\"MODULE {module_name}\")\n",
    "        \n",
    "        for p_name, p in module.named_parameters():\n",
    "            print(f\"{ident}PARAM {p_name} {list(p.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     vision_model.embeddings.class_embedding torch.Size([768])\n",
      "3       vision_model.embeddings.patch_embedding.weight torch.Size([768, 3, 32, 32])\n",
      "3       vision_model.embeddings.position_embedding.weight torch.Size([50, 768])\n",
      "2     vision_model.pre_layrnorm.weight torch.Size([768])\n",
      "2     vision_model.pre_layrnorm.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.0.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.0.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.0.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.0.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.0.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.0.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.0.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.0.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.0.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.0.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.1.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.1.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.1.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.1.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.1.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.1.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.1.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.1.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.1.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.1.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.2.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.2.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.2.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.2.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.2.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.2.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.2.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.2.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.2.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.2.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.3.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.3.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.3.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.3.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.3.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.3.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.3.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.3.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.3.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.3.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.3.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.3.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.4.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.4.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.4.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.4.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.4.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.4.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.4.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.4.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.4.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.4.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.4.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.4.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.5.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.5.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.5.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.5.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.5.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.5.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.5.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.5.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.5.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.5.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.5.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.5.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.6.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.6.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.6.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.6.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.6.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.6.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.6.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.6.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.6.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.6.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.6.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.6.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.7.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.7.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.7.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.7.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.7.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.7.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.7.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.7.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.7.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.7.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.7.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.7.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.8.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.8.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.8.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.8.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.8.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.8.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.8.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.8.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.8.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.8.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.8.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.8.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.9.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.9.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.9.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.9.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.9.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.9.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.9.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.9.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.9.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.9.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.9.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.9.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.10.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.10.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.10.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.10.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.10.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.10.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.10.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.10.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.10.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.10.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.10.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.10.layer_norm2.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.11.self_attn.k_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.11.self_attn.v_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.11.self_attn.q_proj.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([768, 768])\n",
      "6             vision_model.encoder.layers.11.self_attn.out_proj.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.11.layer_norm1.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.11.layer_norm1.bias torch.Size([768])\n",
      "6             vision_model.encoder.layers.11.mlp.fc1.weight torch.Size([3072, 768])\n",
      "6             vision_model.encoder.layers.11.mlp.fc1.bias torch.Size([3072])\n",
      "6             vision_model.encoder.layers.11.mlp.fc2.weight torch.Size([768, 3072])\n",
      "6             vision_model.encoder.layers.11.mlp.fc2.bias torch.Size([768])\n",
      "5           vision_model.encoder.layers.11.layer_norm2.weight torch.Size([768])\n",
      "5           vision_model.encoder.layers.11.layer_norm2.bias torch.Size([768])\n",
      "2     vision_model.post_layernorm.weight torch.Size([768])\n",
      "2     vision_model.post_layernorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k,v in params.items():\n",
    "    depth = len(k.split('.')) - 1\n",
    "    ident = depth * \"  \"    \n",
    "    print(f\"{depth} {ident}{k} {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "tensors, stats = summary(model, input_data=[pixels], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurips",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
